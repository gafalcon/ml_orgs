
* Machine Learning
** Base
*** Supervise vs unsupervised
*** Bias-Variance tradeoff
*** Bootstrapping
    Bootstrapping is a sampling technique in which we choose ‘n’ observations or rows out of the original dataset of ‘n’ rows as well. But the key is that each row is selected with replacement from the original dataset so that each row is equally likely to be selected in each iteration.
** Data exploration
   [[./exploratory_data_analysis.org]]
*** normalization
**** log transformation
*** TODO heteroscedasticity
**** Box-cox transform
*** TODO Missing data
    - median, mean
**** Imputing
***** TODO MICE 
** Validation/test sets and Cross Validation
   train on a test set, validate predictions in a validation set, and when u have ur model complete, then use test set. Usually select sets in random. But if there is a time feature, not random.
   To calibrate validation set, check if is representative: train several models of different prediction capacity, test them on the validation and on the test, set. The results, set in a scatter plot, test vs val results, the dots should make a line. If not the validation is not close to the test set.
*** Cross Validation
    To not overfit on validation set
    - GridSearch
    - RandomSearch
** Algorithms
*** Regression
**** Linear regression
**** Polynomial regression
**** Logistic regression
**** TODO ElasticNet
**** TODO KernelRidge Regression
*** k-nearest neighbors
    - Finds the k closest neighbors and classify according to the most common class. 
    - Elbow method to find k
    - no training
    - long prediction cuz compares to each training sample
    - different comparing functions: L2, L1, cos
    - Unreliable in many dimensions
*** tree-regression, tree classifier
    - Easy to overfit
*** svm
*** Ensembles
    [[./images/ensemble.png]]
    Combining the predictions of multiple ml models. Unweighted avg of the models' predictions, or weighted depending on the accuracy of each model, or Majority Voting
    [[./images/weighted-unweighted.png]]
**** random forest
     Bagging to create multiple decision trees, which reduces variance. Also chooses only subset of features. Later aggregates the predictions of individual trees.
     Doesnt overfit much. 
     Doesnt have statistic assumptions.
     [[./trees.org]]
***** sklearn ensemble RandomForestClassifier or RandomForestRegressor
      - params: n_estimators, max_depth, bootstrap, n_jobs
      - oob_score (out of bag score): calculate error on training set, but only include trees in the calculation of a row's error where that row was not included in training that tree. This allows us to see whether the model is over-fitting, without needing a separate val set. Good if we have small amount of data. The score is a little less good because it doesnt use to whole forest to calculate the oob, just a subset.
***** sklearn ExtraTrees
      Extreme Randomized Trees. Random select splits and features. To make more uncorrelated trees in the forest.
       
**** boosting
     [[./boosting.org]]
**** Bagging (Bootstrap Aggregation)
     is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. Helps reduce variance.
     Bagging gets around overfitting by creating it’s own variance amongst the data by sampling and replacing data (Bootstrapping) while it tests multiple hypothesis(models). In turn, this reduces the noise by utilizing multiple samples that would most likely be made up of data with various attributes(median, average, etc).

     Once each model has developed a hypothesis. The models use voting for classification or averaging for regression. This is where the “Aggregating” in “Bootstrap Aggregating” comes into play. Each hypothesis has the same weight as all the others. 
**** Stacking
     [[./stacking.org]]
**** Disadvantages
     Ensembling reduces the model interpretability and makes it very difficult to draw any crucial business insights at the end.
     It is time-consuming and thus might not be the best idea for real-time applications.
     The selection of models for creating an ensemble is an art which is really hard to master.
*** Recommender systems
    Content-based filtering
    Collaborative filtering
*** Naive Bayes
*** Unsupervised Methods
**** Clustering
***** TODO GMM (Gaussian Mixture models)
***** k-means
***** TODO dbscan
***** TODO hierarchical clustering
**** Dimensionality Reduction
***** PCA
***** TODO LDA
***** TODO SVD
** TODO Loss functions
*** LogLoss
    Quantifies accuracy by penalising false classifications. Must assign a probability to each class rather than simply yielding the most likely class.
    [[./images/logloss.png]]
    N is num of examples, M num of possible labels, y_{ij} is binary indicator of whether or not label j is correct classification for instance i, and p_{ij} is the model probability of assigning label j to instance i.
    For binary classification:
    [[./images/logloss_binary.png]]
    Log Loss penalises classifiers that are confident about an incorrect classification. If the classifier assigns a very small prob to the correct class then the Log Loss will be very large. 
    It's better to be somewhat wrong than emphatically wrong.
    [[./images/logloss_curve.png]]
*** Multiclass Support Vector Machine (SVM) loss. Hinge loss
    The correct class must have score higher than the incorrect classes by some fixed margin Delta. Delta can be safely set at 1.0 in all cases. the \lambda is the one to take into account
    - is more local objective. As long as the correct class is higher than the rest by the margin specified, the loss will be zero. [10,8,8] would be the same as [10, -100, -100] where the first one is the correct class.
*** Softmax
**** Hierarchical softmax
*** TODO Huber loss
** Optimization
*** Hyperparameter tuning
**** Cross-validation
     - Grid search: select combination of hyperparameters to find which combination works better
     - Random search: instead of trying out all possible combinations, it evaluates a given num of random combinations by selecting a random value for each hyperparam at every iteration. Preferred if you have lots of hyperparams.
     -  
** Inspection
*** Confusion matrix
    compares predictions with the true label. To check false positives and false negatives
    - sklearn.metrics.confusion_matrix(true_values, predicted_values)
    - sns.heatmap(c_matrix, annot=True)
*** Most important features.
    - In randomforest there is a method. Crude and static in the sense that it gives little insight in understanding individual decisions on actual data.
    - In regression, features with highest weights. They can be very biased.
    - Word2Vec: *Lime* 
    - LIME: allows users to explain the decisions of any classifier *on one particular example* by perturbing the input and seeing how the prediction changes
*** Partial Dependence
    For black box ml algorithms, useful to understand the relations between predictors and model outcome. Helps to know in which direction a feature influences the outcome.
    - python *pdpbox*
      [[./images/pdp_plot.png]]
      [[./images/pdp_cluster.png]]
    - partial dependence plot aims to visualize the marginal effect of a given predictor towards the model outcome by plotting out the average model outcome in terms of different values of the predictor.
    - Replaces column of interest with constant values, leaving all the other features the same, and records the prediction value. Keep doing the same with different values for the column of interest.
    - Useful also for interactions between features
      [[./images/pdp_interaction_plot.png]]
    - We can see dependencies between categories
      [[./images/pdp_cat.png]]
    - If we want to see how feature A is influencing the prediction Y, what PDP does is to generate a new data set as follow and do prediction as usual. (here we assume that feature A has three unique values: A1, A2, A3)
      [[./images/pdp_table.png]]
*** Tree Interpreter
    To interpret how much each feature contributed to the final outcome.
    a more “operational” way to define the prediction, namely through the sequence of regions that correspond to each node/decision in the tree. Since each decision is guarded by a feature, and the decision either adds or subtracts from the value given in the parent node, the prediction can be defined as the sum of the feature contributions + the “bias” (i.e. the mean given by the topmost region that covers the entire training set).
    f(x)=c_full + \sum_{k=1}^{K}contrib(x,k) where K is the number of features, c_{full} is the value at the root of the node and contrib(x,k) is the contribution from the k-th feature in the feature vector x, which would be difference between the mean value at that branch node and the mean at the parent node. This is superficially similar to linear regression (f(x)=a+bx). For linear regression the coefficients b are fixed, with a single constant for every feature that determines the contribution. For the decision tree, the contribution of each feature is not a single predetermined value, but depends on the rest of the feature vector which determines the decision path that traverses the tree and thus the guards/contributions that are passed along the way.
    - python module: treeinterpreter
    - [[http://blog.datadive.net/interpreting-random-forests/][Interpreting random forests]]
** Comparison
** Resources
   - https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy
   - https://www.analyticsvidhya.com/blog/2017/02/introduction-to-ensembling-along-with-implementation-in-r/
