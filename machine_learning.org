
* Machine Learning
** Base
*** Supervise vs unsupervised
*** Bias-Variance tradeoff
*** Bootstrapping
    Bootstrapping is a sampling technique in which we choose ‘n’ observations or rows out of the original dataset of ‘n’ rows as well. But the key is that each row is selected with replacement from the original dataset so that each row is equally likely to be selected in each iteration.
** Data exploration
   [[./exploratory_data_analysis.org]]
*** normalization
**** log transformation
*** TODO heteroscedasticity
**** Box-cox transform
*** TODO Missing data
    - median, mean
**** Imputing
***** TODO MICE 
** Algorithms
*** Regression
**** Linear regression
**** Polynomial regression
**** Logistic regression
**** TODO ElasticNet
**** TODO KernelRidge Regression
*** k-nearest neighbors
    - Finds the k closest neighbors and classify according to the most common class. 
    - Elbow method to find k
    - no training
    - long prediction cuz compares to each training sample
    - different comparing functions: L2, L1, cos
    - Unreliable in many dimensions
*** tree-regression, tree classifier
    - Easy to overfit
*** svm
*** Ensembles
    Combining the predictions of multiple ml models. Unweighted avg of the models' predictions, or weighted depending on the accuracy of each model, or Majority Voting
    [[./images/weighted-unweighted.png]]
**** random forest
     Bagging to create multiple decision trees, which reduces variance. Also chooses only subset of features. Later aggregates the predictions of individual trees.
**** boosting
     [[./boosting.org]]
**** Bagging (Bootstrap Aggregation)
     is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. Helps reduce variance.
     Bagging gets around overfitting by creating it’s own variance amongst the data by sampling and replacing data (Bootstrapping) while it tests multiple hypothesis(models). In turn, this reduces the noise by utilizing multiple samples that would most likely be made up of data with various attributes(median, average, etc).

     Once each model has developed a hypothesis. The models use voting for classification or averaging for regression. This is where the “Aggregating” in “Bootstrap Aggregating” comes into play. Each hypothesis has the same weight as all the others. 
**** Stacking
     [[./stacking.org]]
**** Disadvantages
     Ensembling reduces the model interpretability and makes it very difficult to draw any crucial business insights at the end.
     It is time-consuming and thus might not be the best idea for real-time applications.
     The selection of models for creating an ensemble is an art which is really hard to master.
*** Recommender systems
    Content-based filtering
    Collaborative filtering
*** Naive Bayes
*** Unsupervised Methods
**** Clustering
***** TODO GMM (Gaussian Mixture models)
***** k-means
***** TODO dbscan
***** TODO hierarchical clustering
**** Dimensionality Reduction
***** PCA
***** TODO LDA
***** TODO SVD
** TODO Loss functions
*** LogLoss
    Quantifies accuracy by penalising false classifications. Must assign a probability to each class rather than simply yielding the most likely class.
    [[./images/logloss.png]]
    N is num of examples, M num of possible labels, y_{ij} is binary indicator of whether or not label j is correct classification for instance i, and p_{ij} is the model probability of assigning label j to instance i.
    For binary classification:
    [[./images/logloss_binary.png]]
    Log Loss penalises classifiers that are confident about an incorrect classification. If the classifier assigns a very small prob to the correct class then the Log Loss will be very large. 
    It's better to be somewhat wrong than emphatically wrong.
    [[./images/logloss_curve.png]]
*** Multiclass Support Vector Machine (SVM) loss. Hinge loss
    The correct class must have score higher than the incorrect classes by some fixed margin Delta. Delta can be safely set at 1.0 in all cases. the \lambda is the one to take into account
    - is more local objective. As long as the correct class is higher than the rest by the margin specified, the loss will be zero. [10,8,8] would be the same as [10, -100, -100] where the first one is the correct class.
*** Softmax
**** Hierarchical softmax
*** TODO Huber loss
** Optimization
*** Hyperparameter tuning
**** Cross-validation
     - Grid search: select combination of hyperparameters to find which combination works better
     - Random search: instead of trying out all possible combinations, it evaluates a given num of random combinations by selecting a random value for each hyperparam at every iteration. Preferred if you have lots of hyperparams.
     -  
** Inspection
*** Confusion matrix
    compares predictions with the true label. To check false positives and false negatives
*** Most important features.
    - In randomforest there is a method.
    - In regression, features with highest weights.
    - Word2Vec: *Lime* 
    - LIME: allows users to explain the decisions of any classifier *on one particular example* by perturbing the input and seeing how the prediction changes
** Comparison
** Resources
   - https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy
   - https://www.analyticsvidhya.com/blog/2017/02/introduction-to-ensembling-along-with-implementation-in-r/
