* Decision Trees
  - from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
  - Require very little data preparation. Dont require feature scaling or centering at all.
  - Scikit-Learn uses CART algorithm, which produces only /binary trees/. Other algorithms such as ID3 can have nodes with more than two children
  - A decision tree can also estimate class probability of class k: returns ration of training instances of class k in the leaf node of an instance.
  - For classification (Gini index or Entropy). For regression (mse)
  - For regression, prediction is simply the average target value of n training instances associated to a leaf node.
  - Prone to *overfitting*
** Gini index
   Measure node /impurity/: a node is "pure" (gini=0) if all training instances it applies to belong to the same class. 
   [[./images/gini_index.png]]
** Cart (Classification and Regression Tree) Algorithm
   The algorithm first splits the training set in two subsets using a single feature k and a threshold t_k. How? It searches for the pair (k, t_{k}) that produces the purest subsets (weighted by their size)(Using purity if classification, or mse if regression).  The cost function to minimize is 
   [[./images/cart.png]]

   Using the same logic, it keeps spliting the subsets and so on, recursively. Stops when reaching maximum depth, or if it cannot find a split that will reduce impurity.
   
   It is a greedy algorithm, it searches for an optimum split at the top level, and repeats the process without checking if the split will lead to the lowest possible impurity several levels down. It is not guaranteed to find the optimal solution. Finding the optimal is an NP-Complete problem.
** Computational Complexity
   - Prediction time: O(log_{2}(m)), independent of the num of features. Prediction very fast.
   - Training time: compares all features on all samples at each node. O(nxmlog(m)).
** Entropy
   Another impurity measure. Is zero when a node contains instances of only one class. Measures the average information content of a message.
   [[./images/entropy.png]]
   They lead to similar trees as the ones created using Gini. Gini is slightly faster. When differ, Entropy tends to produce slightly more balanced trees.
** Hyperparameters
   Increasing min_* hyperparams or reducing max_* hyperparams will regularize the model.
   - max_depth
   - min_samples_split
   - min_samples_leaf
   - max_features
** Instability
   Decision Trees love orthogonal decision boundaries(all splits are perpendicular to an axis), which makes them sensitive to training set rotation)
   [[./images/training_rotation.png]]
   One way to limit this problem is using PCA, which often results in better orientation of the training data.
   
   Very sensitive to small variations in training data. Random forests can limit this instability by averaging predictions over many trees.
* Random Forest
  Ensemble of Decision Trees, trained via bagging method, typically with max_samples set to size of the training set.
  Rather than searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in greater tree diversity, which trades a higher bias for a lower variance.
* Extra-Trees (Extremely Randomized Trees)
  It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds. Trades more bias for lower variance.
  Much faster to train than regular Random Forests.
  Generally, try both extra-trees and random forest and cross-validate to see which performs better.
* Feature Importance
  Looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). It is a weighted average, where each node's weight is equal to the number of training samples that are associated with it.

  If there are colinear features, they may share feature importance*
* Confidence Based on tree Variance
Use the standard deviation of predictions, instead of just the mean. This tells us the relative confidence of predictions. For rows where the trees give very differente results, you would want to be more cautious of using those results, compared to cases where they are more consistent.
