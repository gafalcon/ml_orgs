* Boosting
  Family of algorithms that convert collection of weak learners to strong learners
  - using avg/weighted avg
  - considering precision has higher vote
  in boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.

  It’s really important to note that boosting is focused on reducing the bias. This makes the boosting algorithms prone to overfitting. Thus, parameter tuning becomes a crucial part of boosting algorithms to make them avoid overfitting.
** Basic Procedure
   1. Start with set of classifiers h_1, ..., h_k
   2. Create a classifier that combines classification functions f(x) = sgn \sum^T \alpha_{t}h_{t}(x)
      - Goal is to minimize error (on training set)
      - Iterative, select one h at each step
      - Calculate weights based on errors
      - Upweight missed classifications and select next h
   Boosting pays higher focus on examples which are miss-classified or have higher errors by preceding weak rules. Place the most weight on the examples most often missclassified by the preceding weak rules; this has the effect of forcing the base learner to focus its attention on the "hardest" examples.

** AdaBoost (Adaptive Boosting)
   Fits sequence of weak learners on diff weighted training data. Starts predicting original data set and gives equal w to each observation. If pred is incorrect using fst learner, then it gives higher weight to observation which have been predicted incorrectly. Continues to add learners iteratively until a limit is reached in the num of models or accuracy.
   Mostly uses decision stamps, but can use any machine learning algorithm as base learner if it accepts weight on trainind dataset.
   
   [[./org_images/adaboost.png]]

  - AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. 
  - AdaBoost is sensitive to noisy data and outliers. In some problems it can be less susceptible to the overfitting problem than other learning algorithms. 
  - The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing, the final model can be proven to converge to a strong learner.
  - AdaBoost (with decision trees as the weak learners) is often referred to as the best out-of-the-box classifier.When used with decision tree learning, information gathered at each stage of the AdaBoost algorithm about the relative 'hardness' of each training sample is fed into the tree growing algorithm such that later trees tend to focus on harder-to-classify examples.
  - the AdaBoost training process selects only those features known to improve the predictive power of the model, reducing dimensionality and potentially improving execution time as irrelevant features need not be computed.

  The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights w_1, w_2, …, w_N to each of the training samples. Initially, those weights are all set to w_i = 1/N, so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence

  - The main parameters to tune to obtain good results are n_estimators and the complexity of the base estimators (e.g., its depth max_depth or minimum required number of samples at a leaf min_samples_leaf in case of decision trees).
  - Boosting seems to be susceptible to noise.
  - Ability to find outliers.  Because AdaBoost focuses its weight on the hardest examples, the examples with the highest weight often turn out to be outliers.
  - When the number of outliers is very large, the emphasis placed on the hard examples can become detrimental to the performance of AdaBoost.
    - Alternatives to handle outliers : *Gentle Adaboost, BrownBoost*
** Gradient Boosting
   trains many model sequentially. Each new model gradually minimizes the loss function (y = ax + b + e, e needs special attention as it is an error term) of the whole system using Gradient Descent method. The learning procedure consecutively fit new models to provide a more accurate estimate of the response variable.

   The principle idea behind this algorithm is to construct new base learners which can be maximally correlated with negative gradient of the loss function, associated with the whole ensemble.  

   The advantages of GBRT are:
   - Natural handling of data of mixed type (= heterogeneous features)
   - Predictive power
   - Robustness to outliers in output space (via robust loss functions)

   The disadvantages of GBRT are: Scalability, due to the sequential nature of boosting it can hardly be parallelized.

   - Smaller values of learning_rate require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of learning_rate favor better test error.Recommend to set the learning rate to a small constant (e.g. learning_rate <= 0.1) and choose n_estimators by early stopping
** XGBoost Xtreme Gradient Boosting
   - Efficient: Automatic parallel computation on a single machine. Can be run on a cluster
   - Accuracy: Good result for most data sets.
   - Feasible: Customized objective and evaluation. Tunable params.
*** Params
    1. General Params
       - num of threads
    2. Booster params
       - stepsize
       - regularization
       - max depth
    3. Task params
       - Objective
       - Evaluation metric
    
    Can group them as:
    - Controlling complexity: *max-depth, min-child-weight, gamma*
    - Robust to noise: *subsample, colsample-bytree*
    
    Sometimes data is imbalanced among classes.
    - Only care about the ranking order
      - Balance the positive and negative  weights, by *scale-pos-weight*
      - Use "auc" as the evaluation metric.
    - Care about predicting the right probability
      - Cannot re-balance the dataset
      - Set *max-delta-step* to finite num (1) will help convergence.
    
    - Use *early.stop.round* to detect continuously being worse on test set.
    - If overfitting, reduce stepsize *eta* and increase *nround* at the same time
*** Advanced Features
    - Customized objective and evaluation metric
    - Prediction from cross validation
    - Continue training on existing model
    - Calculate and plot the variable importance

** References
 - http://scikit-learn.org/stable/modules/ensemble.html#ensemble
 - https://mlwave.com/kaggle-ensembling-guide/
 - https://arxiv.org/abs/1603.02754 - xgboost paper
